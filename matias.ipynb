{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import nltk\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from nltk.tokenize import word_tokenize\n",
    "# Read the text file\n",
    "with open('/teamspace/studios/this_studio/Project-III/data/sherlock-holm.es_stories_plain-text_advs.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "text = text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in vocabulary: 8384\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the text \n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "\n",
    "vocabulary = set(tokens)\n",
    "\n",
    "\n",
    "total_words = len(vocabulary) + 1  \n",
    "\n",
    "# Create a word-to-index mapping\n",
    "word_to_idx = {word: idx for idx, word in enumerate(vocabulary)}\n",
    "\n",
    "print(f\"Total words in vocabulary: {total_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(text, seq_length, word_to_idx, pad_value=0):\n",
    "  \"\"\"\n",
    "  This function creates sequences of word indices from the text for training,\n",
    "  with optional padding.\n",
    "\n",
    "  Args:\n",
    "      text: The input text string.\n",
    "      seq_length: The desired sequence length (number of words in each sequence).\n",
    "      word_to_idx: Dictionary mapping words to their numerical indices.\n",
    "      pad_value: The value to use for padding (default: 0).\n",
    "\n",
    "  Returns:\n",
    "      A list of lists, where each inner list represents a sequence of word indices.\n",
    "  \"\"\"\n",
    "\n",
    "  sequences = []\n",
    "  tokens = word_tokenize(text.lower())  # Tokenize the text (lowercase)\n",
    "\n",
    "  for i in range(len(tokens) - seq_length + 1):\n",
    "    sequence = tokens[i:i + seq_length]\n",
    "    sequence_indices = [word_to_idx[word] for word in sequence]  # Convert words to indices\n",
    "\n",
    "    # Pad the sequence if necessary\n",
    "    if len(sequence_indices) < seq_length:\n",
    "      sequence_indices.extend([pad_value] * (seq_length - len(sequence_indices)))\n",
    "\n",
    "    sequences.append(sequence_indices)\n",
    "\n",
    "  return sequences\n",
    "\n",
    "  \n",
    "def process_sequences(text, word_to_idx, seq_length=None, unknown_token=\"me\"):\n",
    "  \"\"\"\n",
    "  This function processes text into sequences of word indices, handling unknown words with a special token.\n",
    "\n",
    "  Args:\n",
    "      text: The input text string.\n",
    "      word_to_idx: Dictionary mapping words to their numerical indices.\n",
    "      seq_length: The desired sequence length (number of words in each sequence).\n",
    "      unknown_token: The token to use for unknown words (default: \"<unk>\").\n",
    "\n",
    "  Returns:\n",
    "      A PyTorch tensor containing padded sequences of word indices.\n",
    "  \"\"\"\n",
    "\n",
    "  words = text.split()  # Split text into words\n",
    "\n",
    "  # Preprocess words: remove periods, replace double dashes with single space\n",
    "  words = [word.rstrip('.').replace('--', ' ') for word in words]\n",
    "  words = [word.rstrip('.') for word in words]\n",
    "\n",
    "  # Replace unknown words with the special token\n",
    "  words = [word_to_idx.get(word, word_to_idx[unknown_token]) for word in words]\n",
    "\n",
    "  indices = torch.tensor(words)  # Convert words to indices directly (using tensor)\n",
    "\n",
    "  # Pad sequences if necessary (assuming batch size of 1)\n",
    "  if seq_length is not None:\n",
    "    padded_sequences = pad_sequence([indices], batch_first=True, padding_value=0)\n",
    "  else:\n",
    "    padded_sequences = indices\n",
    "\n",
    "  return padded_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 50\n",
    "\n",
    "\n",
    "sequences = create_sequences(text, seq_length, word_to_idx)\n",
    "\n",
    "\n",
    "# Convert sequences to a PyTorch tensor\n",
    "sequences_tensor = torch.tensor(sequences)\n",
    "\n",
    "# Pad sequences using pad_sequence with batch_first=True\n",
    "padded_sequences = pad_sequence(sequences_tensor, batch_first=True, padding_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input (X) and target labels (y)\n",
    "X = padded_sequences[:, :-1]  # All rows, all columns except last\n",
    "y = padded_sequences[:, -1]  # All rows, only last column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "  def __init__(self, vocab_size, embedding_dim, hidden_dim, output_size):\n",
    "    super(LSTMModel, self).__init__()\n",
    "    # Embedding layer to map word indices to embeddings\n",
    "    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "    # LSTM layer with hidden units\n",
    "    self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "    # Output layer to predict next word\n",
    "    self.output = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "  def forward(self, x):\n",
    "    # Pass input sequence (x) through the embedding layer\n",
    "    embedded = self.embedding(x)\n",
    "    # Pass embedded sequence through the LSTM layer\n",
    "    lstm_out, _ = self.lstm(embedded)\n",
    "    # Get the output from the last hidden state of the LSTM\n",
    "    output = lstm_out[:, -1, :]  # Select last hidden state\n",
    "    # Pass the output through the linear layer to get logits\n",
    "    prediction = self.output(output)\n",
    "    return prediction\n",
    "\n",
    "\n",
    "vocab_size = len(word_to_idx)  # Get the number of unique words\n",
    "embedding_dim = 128\n",
    "hidden_dim = 128\n",
    "output_size = len(word_to_idx)\n",
    "\n",
    "\n",
    "model = LSTMModel(vocab_size, embedding_dim, hidden_dim, output_size)\n",
    "\n",
    "# Pass input sequence (x) through the model\n",
    "processed_sequences = process_sequences(text, word_to_idx, seq_length=50)  # Example with padding\n",
    "\n",
    "x = processed_sequences\n",
    "predictions = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(torch.utils.data.Dataset):\n",
    "  def __init__(self, text, word_to_idx, seq_length=50, unknown_token=\"me\"):\n",
    "    self.text = text\n",
    "    self.word_to_idx = word_to_idx\n",
    "    self.seq_length = seq_length\n",
    "    self.unknown_token = unknown_token\n",
    "\n",
    "  def __len__(self):\n",
    "    # Calculate the number of sequences based on text length and sequence length\n",
    "    num_sequences = int(len(self.text) // self.seq_length)\n",
    "    return num_sequences\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    # Get the starting index for the current sequence\n",
    "    start_idx = idx * self.seq_length\n",
    "\n",
    "    # Get the sequence of words from the text\n",
    "    words = self.text[start_idx: start_idx + self.seq_length].split()\n",
    "\n",
    "    # Preprocess words (remove periods, replace double dashes)\n",
    "    words = [word.rstrip('.').replace('--', ' ') for word in words]\n",
    "\n",
    "    # Replace unknown words with the special token\n",
    "    words = [self.word_to_idx.get(word, self.word_to_idx[self.unknown_token]) for word in words]\n",
    "\n",
    "    # Convert words to PyTorch tensor\n",
    "    sequence = torch.tensor(words)\n",
    "\n",
    "    # Get the target label (next word)\n",
    "    # Assuming next word is the word after the sequence\n",
    "    if len(words) == self.seq_length:\n",
    "      label = words[-1]  # Last word in the sequence is the label\n",
    "    else:\n",
    "      label = self.word_to_idx[self.unknown_token]  # If sequence is shorter, use unknown token as label\n",
    "\n",
    "    # Convert label to PyTorch tensor\n",
    "    label = torch.tensor(label)\n",
    "\n",
    "    # Return a dictionary with sequence (x) and label (y)\n",
    "    return {'x': sequence, 'y': label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_collate(batch):\n",
    "  \"\"\"\n",
    "  Custom collate function to pad sequences within a batch.\n",
    "\n",
    "  Args:\n",
    "      batch: A list of dictionaries containing sequences (x) and labels (y).\n",
    "\n",
    "  Returns:\n",
    "      A dictionary with padded sequences (x) and a list of labels (y).\n",
    "  \"\"\"\n",
    "\n",
    "  # Get the maximum sequence length within the batch\n",
    "  max_len = max(len(data['x']) for data in batch)\n",
    "\n",
    "  # Handle the case where all sequences in the batch have the same length (edge case)\n",
    "  if max_len == 0:\n",
    "    max_len = 1  # Set a minimum length to avoid padding issues\n",
    "\n",
    "  # Pad sequences with a special padding token (e.g., 0)\n",
    "  padded_sequences = [\n",
    "      torch.nn.functional.pad(data['x'], pad=(0, max_len - len(data['x'])), value=0)\n",
    "      for data in batch\n",
    "  ]\n",
    "\n",
    "  # Get a list of labels\n",
    "  labels = [data['y'] for data in batch]\n",
    "\n",
    "  # Convert padded sequences and labels to tensors\n",
    "  padded_sequences = torch.stack(padded_sequences)\n",
    "  labels = torch.tensor(labels)\n",
    "\n",
    "  return {'x': padded_sequences, 'y': labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10\n",
      "Training Loss: 0.3567\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [5] at entry 0 and [10] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 51\u001b[0m\n\u001b[1;32m     48\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():  \u001b[38;5;66;03m# Disable gradient calculation during validation\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m validation_loader:\n\u001b[1;32m     52\u001b[0m     sequences \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     53\u001b[0m     labels \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:277\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    217\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:129\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collections\u001b[38;5;241m.\u001b[39mabc\u001b[38;5;241m.\u001b[39mMapping):\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 129\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m elem_type({key: collate([d[key] \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m batch], collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m elem})\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    131\u001b[0m         \u001b[38;5;66;03m# The mapping type may not support `__init__(iterable)`.\u001b[39;00m\n\u001b[1;32m    132\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m {key: collate([d[key] \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m batch], collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m elem}\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:129\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collections\u001b[38;5;241m.\u001b[39mabc\u001b[38;5;241m.\u001b[39mMapping):\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 129\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m elem_type({key: \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m elem})\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    131\u001b[0m         \u001b[38;5;66;03m# The mapping type may not support `__init__(iterable)`.\u001b[39;00m\n\u001b[1;32m    132\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m {key: collate([d[key] \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m batch], collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m elem}\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:121\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 121\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    124\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:174\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    172\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    173\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[0;32m--> 174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [5] at entry 0 and [10] at entry 1"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define hyperparameters (adjust as needed)\n",
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "\n",
    "\n",
    "# Create training and validation datasets\n",
    "train_dataset = TextDataset(text, word_to_idx, seq_length=50)\n",
    "validation_dataset = TextDataset(text, word_to_idx, seq_length=50)  # Assuming a split for validation\n",
    "\n",
    "# Create data loaders for training and validation\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=pad_collate)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=batch_size)\n",
    "\n",
    "# Define model (assuming you have the LSTMModel class defined)\n",
    "model = LSTMModel(vocab_size=len(word_to_idx), embedding_dim=128, hidden_dim=128, output_size=len(word_to_idx))\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "  print(f'Epoch: {epoch+1}/{num_epochs}')\n",
    "\n",
    "  # Training phase\n",
    "  model.train()  # Set model to training mode\n",
    "  train_loss = 0.0\n",
    "\n",
    "  for data in train_loader:\n",
    "    sequences = data['x']  # Get sequences (x) from the data dictionary\n",
    "    labels = data['y']  # Get labels (y) from the data dictionary\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    predictions = model(sequences)\n",
    "    loss = criterion(predictions, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    train_loss += loss.item()  # Accumulate training loss\n",
    "\n",
    "  # Calculate and print average training loss per epoch\n",
    "  avg_train_loss = train_loss / len(train_loader)\n",
    "  print(f'Training Loss: {avg_train_loss:.4f}')\n",
    "\n",
    "  # Optional: Validation phase\n",
    "  model.eval()  # Set model to evaluation mode\n",
    "  val_loss = 0.0\n",
    "\n",
    "  with torch.no_grad():  # Disable gradient calculation during validation\n",
    "    for data in validation_loader:\n",
    "      sequences = data['x']\n",
    "      labels = data['y']\n",
    "\n",
    "      predictions = model(sequences)\n",
    "      loss = criterion(predictions, labels)\n",
    "      val_loss += loss.item()\n",
    "\n",
    "  # Calculate and print average validation loss per epoch\n",
    "  avg_val_loss = val_loss / len(validation_loader)\n",
    "  print(f'Validation Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "  print('-' * 50)  # Separator for each epoch\n",
    "\n",
    "# Save the trained model (optional)\n",
    "torch.save(model.state_dict(), 'next_word_prediction_model.pt')\n",
    "\n",
    "print('Training complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate our predicitons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
