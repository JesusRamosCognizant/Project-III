{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import list\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau  # Import ReduceLROnPlateau\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project-III/data/sherlock-holm.es_stories_plain-text_advs.txt\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                        the adventures of sherlock holmes\n",
      "\n",
      "                               arthur conan doyle\n",
      "\n",
      "\n",
      "\n",
      "                                table of contents\n",
      "\n",
      "               a scandal in bohem\n"
     ]
    }
   ],
   "source": [
    "# Different Editors paths\n",
    "PATH_A = \"data\\sherlock-holm.es_stories_plain-text_advs.txt\"\n",
    "PATH_E = \"\"\n",
    "PATH_G = \"0. Projects/3/Project-III/data/sherlock-holm.es_stories_plain-text_advs.txt\"\n",
    "PATH_J = \"\"\n",
    "PATH_M = \"Project-III/data/sherlock-holm.es_stories_plain-text_advs.txt\"\n",
    "\n",
    "PATHS = [PATH_A, PATH_E, PATH_G, PATH_J, PATH_M]\n",
    "\n",
    "text = \"\"\n",
    "\n",
    "for path in PATHS:\n",
    "    try:\n",
    "        # Read the text file\n",
    "        with open(path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "            print(path)\n",
    "    except:\n",
    "        continue\n",
    "    else:\n",
    "        break\n",
    "\n",
    "\n",
    "\n",
    "print(text[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide the set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '',\n",
       " '',\n",
       " '',\n",
       " '                        the adventures of sherlock holmes',\n",
       " '',\n",
       " '                               arthur conan doyle',\n",
       " '',\n",
       " '',\n",
       " '']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Divide with regular expressions\n",
    "DIVIDERS_ORIGINAL = \"\\n\"\n",
    "DIVIDERS_ALL = \"[,.!?:;\\\"]|\\n\\n|--| and | that | which \"\n",
    "DIVIDERS_MIN = \"[.!]|\\n\\n\"\n",
    "DIVIDERS_BAL = \"[,.!?]|\\n\\n|--\"\n",
    "CLEAR_COVER = False\n",
    "\n",
    "text_try = text.lower()\n",
    "\n",
    "if CLEAR_COVER:\n",
    "    # Delete cover of book and extra information\n",
    "    text_try = text[980:-550]\n",
    "    \n",
    "\n",
    "# Split following the dividers given\n",
    "text_try = re.split(DIVIDERS_ORIGINAL, text_try)\n",
    "\n",
    "# Delete all the new line comments \n",
    "text_try = [el.replace('\\n', '') for el in text_try]\n",
    "\n",
    "text_try[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Tokenizer object in python\n",
    "CLEAR_COVER\n",
    "tokens = word_tokenize(text)\n",
    "vocabulary = set(tokens)\n",
    "total_words = len(vocabulary) + 1\n",
    "\n",
    "word_to_idx = {word:idx for idx, word in enumerate(vocabulary)}\n",
    "\n",
    "print(f\"total_words: {total_words}\")\n",
    "print(f\"√çndice de palabras: {word_to_idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create input-output sequences\n",
    "input_sequences = []\n",
    "for line in text_try:\n",
    "    line_list = line.rstrip(\",.;:\").split(' ')\n",
    "\n",
    "    # Tokenize each sentence\n",
    "    token_list = []\n",
    "    for char in line_list:\n",
    "        if char in word_to_idx.keys():\n",
    "            token_list.append(word_to_idx[char])\n",
    "\n",
    "    # Divide the different sentences in n-grams\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "\n",
    "input_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the max value to add padding to other entries\n",
    "\n",
    "average = 0\n",
    "for seq in input_sequences:\n",
    "    average += len(seq) \n",
    "\n",
    "max_sequence_len, value = max([(len(seq), seq) for seq in input_sequences])\n",
    "input_seq_pad = np.array([np.pad(seq, (max_sequence_len - len(seq), 0), mode='constant') for seq in input_sequences])\n",
    "\n",
    "print (f\"average = {average / len(input_sequences)}\")\n",
    "print (f\"Max seq length = {max_sequence_len}\")\n",
    "input_seq_pad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X and Y separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the sequences into input (X) and output (y)\n",
    "X = input_seq_pad[:, :-1]\n",
    "y = input_seq_pad[:, -1]\n",
    "\n",
    "# Convert output to one-hot encoded vectors\n",
    "y = np.array(torch.nn.functional.one_hot(torch.tensor(y), num_classes=total_words))\n",
    "\n",
    "print (X)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom Dataset class\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.long)\n",
    "        self.y = torch.tensor(y, dtype=torch.float)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "dataset = TextDataset(X, y)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple LSTM layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "class NextWordPredictor(nn.Module):\n",
    "  def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim, dropout=0.2):\n",
    "    super(NextWordPredictor, self).__init__()\n",
    "    self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "    self.dropout_embed = nn.Dropout(dropout)  # Add dropout after embedding\n",
    "    self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers=2, dropout=dropout, bidirectional=True, batch_first=True)  # Use Bidirectional LSTM with multiple layers\n",
    "    self.fc = nn.Linear(hidden_dim * 2, output_dim)  # Adjust output size for Bidirectional LSTM\n",
    "\n",
    "  def forward(self, sequences):\n",
    "    embedded = self.embedding(sequences)\n",
    "    embedded = self.dropout_embed(embedded)\n",
    "    lstm_out, _ = self.lstm(embedded)\n",
    "    last_hidden = lstm_out[:, -1, :]  # Select last hidden state from the sequence\n",
    "    logits = self.fc(last_hidden)\n",
    "    return logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "class NextWordPredictor(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim):\n",
    "        super(NextWordPredictor, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = x[:, -1, :]\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NextWordPredictor(vocab_size = total_words, \n",
    "embed_dim = 100, hidden_dim = 150, output_dim = total_words)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_MODEL = False  #If set to True, saves a torch_model.pt \n",
    "\n",
    "epochs = 200\n",
    "patience = 10  # Number of epochs to wait for improvement\n",
    "current_patience = patience\n",
    "best_loss = float('inf')  # Initialize best loss to a very high value\n",
    "better_model = model\n",
    "for epoch in range(epochs):\n",
    "  # Training loop\n",
    "  for i, (inputs, labels) in enumerate(dataloader):\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, labels.argmax(dim=1))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "  # Early stopping\n",
    "  if loss.item() < best_loss:  # Compare current training loss with best loss\n",
    "    best_loss = loss.item()\n",
    "    print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item()} (Improved)')\n",
    "    better_model = model\n",
    "    current_patience = patience  # Restart patience\n",
    "  else:\n",
    "    print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item()}')\n",
    "    current_patience -= 1  # Decrement patience counter on no improvement\n",
    "\n",
    "  # Stop training if patience is 0\n",
    "  if current_patience == 0:\n",
    "    print('Early stopping triggered!')\n",
    "    break\n",
    "\n",
    "#Saving model to .pt\n",
    "if SAVE_MODEL == True:\n",
    "  torch.save(better_model.state_dict(), 'best_model_simple_LSTM.pt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial text to predict\n",
    "seed_text = \"I am\"\n",
    "next_words = 15\n",
    "\n",
    "# Index to word\n",
    "idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "\n",
    "# Generate the n next words\n",
    "better_model.eval()  # Set the model to evaluation\n",
    "for _ in range(next_words):\n",
    "    tokens = word_tokenize(seed_text)\n",
    "    token_list = [word_to_idx[word] for word in tokens if word in word_to_idx]\n",
    "    token_list = np.pad(token_list, (max_sequence_len - len(token_list), 0), mode='constant')\n",
    "    token_list = torch.tensor(token_list[-max_sequence_len:], dtype=torch.long).unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predicted = better_model(token_list).argmax(dim=1).item()\n",
    "\n",
    "    output_word = idx_to_word[predicted]\n",
    "    seed_text += \" \" + output_word\n",
    "\n",
    "\n",
    "print(seed_text)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
