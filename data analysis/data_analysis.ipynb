{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import list\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from path \n",
    "\n",
    "PATH = \"0. Projects/3/Project-III/data/sherlock-holm.es_stories_plain-text_advs.txt\"\n",
    "\n",
    "# Read the text file\n",
    "with open(PATH, 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "\n",
    "text = text.lower()\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide the set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Try with different separators in split\n",
    "\n",
    "# for line in text.split('.' and '?' and '!' and ':' and ';'):\n",
    "# for line in text.split'\\n'):\n",
    "# for line in text.split('.'):\n",
    "#     print(tokenizer.texts_(to_sequences([line])[0])\n",
    "\n",
    "\n",
    "### Try with all replace\n",
    "\n",
    "# text = text.replace('\\n', ' ').replace('!', '.').replace(':', '.').replace('+', ' ')\n",
    "# text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try with regular expressions\n",
    "\n",
    "DIVIDERS_ORIGINAL = \"\\n\"\n",
    "DIVIDERS_ALL = \"[,.!?:;\\\"]|\\n\\n|--| and | that | which \"\n",
    "DIVIDERS_MIN = \"[.!]|\\n\\n\"\n",
    "DIVIDERS_BAL = \"[,.!?]|\\n\\n|--\"\n",
    "divide_set = False\n",
    "\n",
    "text_try = text\n",
    "\n",
    "if divide_set:\n",
    "    # Delete cover of book and extra information\n",
    "    text_try = text[980:-550]\n",
    "\n",
    "# Split following the dividers given\n",
    "text_try = re.split(DIVIDERS_MIN, text_try)\n",
    "\n",
    "# Delete all the new line comments \n",
    "text_try = [el.replace('\\n', '') for el in text_try]\n",
    "\n",
    "text_try"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Tokenizer object in python\n",
    "\n",
    "# tokenizer = Tokenizer()\n",
    "# tokenizer.fit_on_texts([text])\n",
    "# total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    "vocabulary = set(tokens)\n",
    "total_words = len(vocabulary) + 1\n",
    "\n",
    "word_to_idx = {word:idx for idx, word in enumerate(vocabulary)}\n",
    "\n",
    "print(f\"total_words: {total_words}\")\n",
    "print(\"√çndice de palabras:\", word_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create input-output sequences\n",
    "\n",
    "input_sequences = []\n",
    "# for line in text_try:\n",
    "#     token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "#     for i in range(1, len(token_list)):\n",
    "#         n_gram_sequence = token_list[:i+1]\n",
    "#         input_sequences.append(n_gram_sequence)\n",
    "\n",
    "input_sequences = []\n",
    "for line in text_try:\n",
    "    line_list = line.rstrip(\",.;:\").split(' ')\n",
    "\n",
    "    token_list = []\n",
    "    for char in line_list:\n",
    "        if char in word_to_idx.keys():\n",
    "            token_list.append(word_to_idx[char])\n",
    "\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "\n",
    "input_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the max value to add padding to other entries\n",
    "\n",
    "average = 0\n",
    "for seq in input_sequences:\n",
    "    average += len(seq) \n",
    "\n",
    "max_sequence_len, value = max([(len(seq), seq) for seq in input_sequences])\n",
    "\n",
    "# max_sequence_len = max([len(seq) for seq in input_sequences])\n",
    "# input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "input_seq_pad = np.array([np.pad(seq, (max_sequence_len - len(seq), 0), mode='constant') for seq in input_sequences])\n",
    "\n",
    "print (f\"average = {average / len(input_sequences)}\")\n",
    "input_seq_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get index of the longest line\n",
    "\n",
    "# token_list = []\n",
    "# for line in text_try:\n",
    "#     token_list.append(tokenizer.texts_to_sequences([line])[0])\n",
    "\n",
    "# token_list.index(value)\n",
    "\n",
    "\n",
    "# Print longest line\n",
    "\n",
    "# for el in value:\n",
    "#     for word, index in tokenizer.word_index.items():\n",
    "#         if index == el:\n",
    "#             print(word)\n",
    "#             break\n",
    "# len(input_sequences)\n",
    "# input_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate input and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = input_seq_pad[:, :-1]\n",
    "y = input_seq_pad[:, -1]\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
